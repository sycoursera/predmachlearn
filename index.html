<!DOCTYPE html>
<html>

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="description" content="Predmachlearn : Practical Machine Learning - Prediction Assignment Writeup">

    <link rel="stylesheet" type="text/css" media="screen" href="stylesheets/stylesheet.css">

    <title>Predmachlearn</title>
  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          <a id="forkme_banner" href="https://github.com/sycoursera/predmachlearn">View on GitHub</a>

          <h1 id="project_title">Predmachlearn</h1>
          <h2 id="project_tagline">Practical Machine Learning - Prediction Assignment Writeup</h2>

            <section id="downloads">
              <a class="zip_download_link" href="https://github.com/sycoursera/predmachlearn/zipball/master">Download this project as a .zip file</a>
              <a class="tar_download_link" href="https://github.com/sycoursera/predmachlearn/tarball/master">Download this project as a tar.gz file</a>
            </section>
        </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
        <h4>
<a id="practical-machine-learning" class="anchor" href="#practical-machine-learning" aria-hidden="true"><span class="octicon octicon-link"></span></a>Practical Machine Learning</h4>

<h1>
<a id="prediction-assignment-writeup" class="anchor" href="#prediction-assignment-writeup" aria-hidden="true"><span class="octicon octicon-link"></span></a>Prediction Assignment Writeup</h1>

<hr>

<h3>
<a id="data-sets" class="anchor" href="#data-sets" aria-hidden="true"><span class="octicon octicon-link"></span></a>Data sets</h3>

<p>The Data sets were loaded with read.csv() function. Both "NA" and "" (empty) strigns were treates as NA.</p>

<pre><code>test_set = read.csv("pml-testing.csv")
training_set = read.csv("pml-training.csv", na.strings = c("NA", "") )
</code></pre>

<p>There are lot of variables with NA or empty values. I believe the imputing will not improve fit because number of missing values is too large (more than 95%) for some variables, that is we have no enough data representation for these variables.
These columns were removed with following code:</p>

<pre><code># remove columns with number of NA's more than NA_rate
remove_NA_Columns &lt;- function(x, NA_rate){
    NA_cols_del = data.frame(colSums(is.na(x))) / nrow(x)  &gt; NA_rate
    x[,which(!NA_cols_del)]
}

training_set = remove_NA_Columns(training_set, 0.9)
test_set = remove_NA_Columns(test_set, 0.9)
</code></pre>

<h3>
<a id="data-analysis" class="anchor" href="#data-analysis" aria-hidden="true"><span class="octicon octicon-link"></span></a>Data analysis</h3>

<p>Columns in the both data sets belongs to three types:</p>

<ol>
<li>Numerous sensor's readings</li>
<li>Classification-related column:

<ul>
<li>
<strong>"classe"</strong> <em>for training set</em>
</li>
<li>
<strong>"problem_id"</strong> <em>for test set</em>
</li>
</ul>
</li>
<li>Service columns:

<ul>
<li>
<strong>"X"</strong>               <em>- essentially row index</em>
</li>
<li>
<strong>"user_name"</strong>       <em>- user name</em>
</li>
<li>
<strong>"raw_timestamp_part_1"</strong> and <strong>"raw_timestamp_part_2"</strong> <em>- row timestamp</em>
</li>
<li>
<strong>"cvtd_timestamp"</strong>  <em>- more readable for of timestamp</em>
</li>
<li>
<strong>"new_window"</strong> and <strong>"num_window"</strong> <em>- semantics unknown</em>
</li>
</ul>
</li>
</ol>

<p>The <strong>new_window</strong> variable is categorical with only two levels while one level overwhelms over another (98%).</p>

<p>I decided to use only sensor's readings as relevant to the prediction task. Other columns were removed with following code:</p>

<pre><code>removeColumns &lt;- function(x, columns){
    x = x[,setdiff(names(x), columns)]
}

cols_to_del = c("X", "user_name", 
                "raw_timestamp_part_1", "raw_timestamp_part_2", 
                "cvtd_timestamp", 
                "new_window", 
                "num_window")
training_set = removeColumns(training_set, cols_to_del)
test_set = removeColumns(test_set, cols_to_del)
</code></pre>

<hr>

<p><strong>Variance analysis with PCA:</strong></p>

<pre><code>svd1 &lt;- svd(scale(training[,-ncol(training)]))
plot(svd1$d^2/sum(svd1$d^2), pch = 19, xlab = "Singular vector", ylab = "Variance explained")
</code></pre>

<p><img src="pca_variance.png" alt='alt ""'></p>

<h3>
<a id="training-and-validation-sets" class="anchor" href="#training-and-validation-sets" aria-hidden="true"><span class="octicon octicon-link"></span></a>Training and validation sets</h3>

<p>The remaining <strong>training_set</strong> was splitted to training and validation sets using <em>caret</em> package:</p>

<pre><code>library(caret)
inTrain &lt;- createDataPartition(y=training_set$classe,p=0.7, list=FALSE)
training &lt;- training_set[inTrain,]
testing &lt;- training_set[-inTrain,]
dim(training); dim(testing)
</code></pre>

<h3>
<a id="fit-model-with-random-forest" class="anchor" href="#fit-model-with-random-forest" aria-hidden="true"><span class="octicon octicon-link"></span></a>Fit model with Random Forest</h3>

<p>Actually for Random Forest method there is no need for separate cross-validation or test set to get an unbiased estimate of the test set error, but I decided to use additional test set for sanity check.</p>

<pre><code>library(randomForest)
set.seed(8484)
# train 
modFit &lt;- randomForest(classe~ ., data=training)
# Predicting new values
pred = predict(modFit,newdata=testing)
confusionMatrix(pred, testing$classe)
</code></pre>

<p>Train process with default parameters takes about 30 sec.
Model shows out-of-bag error estimation  =0.48% :</p>

<pre><code>Call:
 randomForest(formula = classe ~ ., data = training) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 7

        OOB estimate of  error rate: 0.48%
Confusion matrix:
     A    B    C    D    E class.error
A 3902    1    2    0    1 0.001024066
B   13 2638    7    0    0 0.007524454
C    0   10 2383    3    0 0.005425710
D    0    0   22 2230    0 0.009769094
E    0    1    2    4 2518 0.002772277
</code></pre>

<p>Printed statistics shows accuracy with testing set =0.9986 :</p>

<pre><code>Confusion Matrix and Statistics

Reference
Prediction    A    B    C    D    E
              A 1673    0    0    0    0
              B    1 1138    2    0    0
              C    0    1 1024    2    0
              D    0    0    0  960    0
              E    0    0    0    2 1082

Overall Statistics

Accuracy : 0.9986
95% CI : (0.9973, 0.9994)
No Information Rate : 0.2845
P-Value [Acc &gt; NIR] : &lt; 2.2e-16

Kappa : 0.9983
</code></pre>

<p>The RF Error Rates:</p>

<pre><code>plot(modFit, log="y", main="Random forest error rates")
legend("topright", colnames(modFit$err.rate),col=1:5,fill=1:5)
</code></pre>

<p><img src="RF_err_rate.png" alt='alt ""'></p>

<h3>
<a id="features-analysis" class="anchor" href="#features-analysis" aria-hidden="true"><span class="octicon octicon-link"></span></a>Features analysis:</h3>

<pre><code># Get variable importance and sort more important first
vimp = varImp(modFit)
vimp_order = order(vimp, decreasing = TRUE)
vimp_sorted = as.data.frame(vimp[vimp_order,], row.names=row.names(vimp)[vimp_order])
names(vimp_sorted) = names(vimp)
</code></pre>

<pre><code>&gt; head(vimp_sorted, 10)
   row.names       Overall
roll_belt         854.6597
yaw_belt          607.1912
pitch_forearm     538.0936
magnet_dumbbell_z 516.4016
pitch_belt        473.0256
magnet_dumbbell_y 466.5814
roll_forearm      428.6566
magnet_dumbbell_x 334.5113
roll_dumbbell     293.6822
accel_belt_z      293.6580
</code></pre>

<pre><code># Dotchart of variable importance as measured by a Random Forest
varImpPlot(modFit, cex=0.6, main="Random forest \nvariable importance")
</code></pre>

<p><img src="var_imp.png" alt='alt ""'></p>

<p>Plots for first 4 important variables show rather discriminative projections of clusters on these variables:</p>

<pre><code>featurePlot(x=testing[, vimp_order[1:4]], y=testing$classe, plot="pairs", main="Pairs for 4 most important features")
</code></pre>

<p><img src="Pairs_4_most_imp_var.png" alt='alt ""'></p>

<p>Attempt to fit model using only first 7 most important variables gives decent results: </p>

<pre><code>OOB estimate of  error rate: 1.53%
Accuracy : 0.9869
</code></pre>

<p>Prediction using both all sensor's variables and only four most important give same result on testing set pml-training.csv:</p>

<pre><code>(pred_tst = predict(modFit,newdata=test_set))

1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20
B  A  B  A  A  E  D  B  A  A  B  C  B  A  E  E  A  B  B  B
</code></pre>
      </section>
    </div>

    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
        <p class="copyright">Predmachlearn maintained by <a href="https://github.com/sycoursera">sycoursera</a></p>
        <p>Published with <a href="http://pages.github.com">GitHub Pages</a></p>
      </footer>
    </div>

    

  </body>
</html>

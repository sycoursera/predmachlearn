{"name":"Predmachlearn","tagline":"Practical Machine Learning - Prediction Assignment Writeup","body":"####Practical Machine Learning\r\n\r\n# Prediction Assignment Writeup\r\n\r\n------\r\n\r\n### Data sets\r\n\r\n\r\nThe Data sets were loaded with read.csv() function. Both \"NA\" and \"\" (empty) strigns were treates as NA.\r\n\r\n    test_set = read.csv(\"pml-testing.csv\")\r\n    training_set = read.csv(\"pml-training.csv\", na.strings = c(\"NA\", \"\") )\r\n\r\n\r\nThere are lot of variables with NA or empty values. I believe the imputing will not improve fit because number of missing values is too large (more than 95%) for some variables, that is we have no enough data representation for these variables.\r\nThese columns were removed with following code:\r\n\r\n    # remove columns with number of NA's more than NA_rate\r\n    remove_NA_Columns <- function(x, NA_rate){\r\n        NA_cols_del = data.frame(colSums(is.na(x))) / nrow(x)  > NA_rate\r\n        x[,which(!NA_cols_del)]\r\n    }\r\n    \r\n    training_set = remove_NA_Columns(training_set, 0.9)\r\n    test_set = remove_NA_Columns(test_set, 0.9)\r\n\r\n\r\n### Data analysis\r\n\r\nColumns in the both data sets belongs to three types:\r\n\r\n1. Numerous sensor's readings\r\n2. Classification-related column:\r\n    + **\"classe\"** *for training set*\r\n    + **\"problem_id\"** *for test set*\r\n3. Service columns:\r\n    + **\"X\"**               *- essentially row index*\r\n    + **\"user_name\"**       *- user name*\r\n    + **\"raw_timestamp_part_1\"** and **\"raw_timestamp_part_2\"** *- row timestamp*\r\n    + **\"cvtd_timestamp\"**  *- more readable for of timestamp*\r\n    + **\"new_window\"** and **\"num_window\"** *- semantics unknown*\r\n    \r\n\r\nThe **new_window** variable is categorical with only two levels while one level overwhelms over another (98%).\r\n\r\nI decided to use only sensor's readings as relevant to the prediction task. Other columns were removed with following code:\r\n\r\n\r\n    removeColumns <- function(x, columns){\r\n        x = x[,setdiff(names(x), columns)]\r\n    }\r\n    \r\n    cols_to_del = c(\"X\", \"user_name\", \r\n                    \"raw_timestamp_part_1\", \"raw_timestamp_part_2\", \r\n                    \"cvtd_timestamp\", \r\n                    \"new_window\", \r\n                    \"num_window\")\r\n    training_set = removeColumns(training_set, cols_to_del)\r\n    test_set = removeColumns(test_set, cols_to_del)\r\n\r\n\r\n\r\n------\r\n**Variance analysis with PCA:**\r\n\r\n    svd1 <- svd(scale(training[,-ncol(training)]))\r\n    plot(svd1$d^2/sum(svd1$d^2), pch = 19, xlab = \"Singular vector\", ylab = \"Variance explained\")\r\n\r\n![alt \"\"](pca_variance.png)\r\n\r\n\r\n### Training and validation sets\r\n\r\nThe remaining **training_set** was splitted to training and validation sets using *caret* package:\r\n\r\n    library(caret)\r\n    inTrain <- createDataPartition(y=training_set$classe,p=0.7, list=FALSE)\r\n    training <- training_set[inTrain,]\r\n    testing <- training_set[-inTrain,]\r\n    dim(training); dim(testing)\r\n\r\n\r\n### Fit model with Random Forest\r\n\r\nActually for Random Forest method there is no need for separate cross-validation or test set to get an unbiased estimate of the test set error, but I decided to use additional test set for sanity check.\r\n\r\n\r\n    library(randomForest)\r\n    set.seed(8484)\r\n    # train \r\n    modFit <- randomForest(classe~ ., data=training)\r\n    # Predicting new values\r\n    pred = predict(modFit,newdata=testing)\r\n    confusionMatrix(pred, testing$classe)\r\n\r\nTrain process with default parameters takes about 30 sec.\r\nModel shows out-of-bag error estimation  =0.48% :\r\n\r\n    Call:\r\n     randomForest(formula = classe ~ ., data = training) \r\n                   Type of random forest: classification\r\n                         Number of trees: 500\r\n    No. of variables tried at each split: 7\r\n    \r\n            OOB estimate of  error rate: 0.48%\r\n    Confusion matrix:\r\n         A    B    C    D    E class.error\r\n    A 3902    1    2    0    1 0.001024066\r\n    B   13 2638    7    0    0 0.007524454\r\n    C    0   10 2383    3    0 0.005425710\r\n    D    0    0   22 2230    0 0.009769094\r\n    E    0    1    2    4 2518 0.002772277\r\n\r\nPrinted statistics shows accuracy with testing set =0.9986 :\r\n\r\n    Confusion Matrix and Statistics\r\n    \r\n    Reference\r\n    Prediction    A    B    C    D    E\r\n                  A 1673    0    0    0    0\r\n                  B    1 1138    2    0    0\r\n                  C    0    1 1024    2    0\r\n                  D    0    0    0  960    0\r\n                  E    0    0    0    2 1082\r\n    \r\n    Overall Statistics\r\n    \r\n    Accuracy : 0.9986\r\n    95% CI : (0.9973, 0.9994)\r\n    No Information Rate : 0.2845\r\n    P-Value [Acc > NIR] : < 2.2e-16\r\n    \r\n    Kappa : 0.9983\r\n\r\n\r\nThe RF Error Rates:\r\n\r\n    plot(modFit, log=\"y\", main=\"Random forest error rates\")\r\n    legend(\"topright\", colnames(modFit$err.rate),col=1:5,fill=1:5)\r\n\r\n![alt \"\"](RF_err_rate.png )\r\n\r\n\r\n### Features analysis:\r\n\r\n\r\n    # Get variable importance and sort more important first\r\n    vimp = varImp(modFit)\r\n    vimp_order = order(vimp, decreasing = TRUE)\r\n    vimp_sorted = as.data.frame(vimp[vimp_order,], row.names=row.names(vimp)[vimp_order])\r\n    names(vimp_sorted) = names(vimp)\r\n\r\n```\r\n> head(vimp_sorted, 10)\r\n   row.names       Overall\r\nroll_belt         854.6597\r\nyaw_belt          607.1912\r\npitch_forearm     538.0936\r\nmagnet_dumbbell_z 516.4016\r\npitch_belt        473.0256\r\nmagnet_dumbbell_y 466.5814\r\nroll_forearm      428.6566\r\nmagnet_dumbbell_x 334.5113\r\nroll_dumbbell     293.6822\r\naccel_belt_z      293.6580\r\n```\r\n\r\n\r\n    # Dotchart of variable importance as measured by a Random Forest\r\n    varImpPlot(modFit, cex=0.6, main=\"Random forest \\nvariable importance\")\r\n\r\n![alt \"\"](var_imp.png)\r\n\r\n\r\nPlots for first 4 important variables show rather discriminative projections of clusters on these variables:\r\n\r\n    featurePlot(x=testing[, vimp_order[1:4]], y=testing$classe, plot=\"pairs\", main=\"Pairs for 4 most important features\")\r\n\r\n\r\n![alt \"\"](Pairs_4_most_imp_var.png)\r\n\r\n\r\n\r\nAttempt to fit model using only first 7 most important variables gives decent results: \r\n\r\n    OOB estimate of  error rate: 1.53%\r\n    Accuracy : 0.9869\r\n\r\n\r\nPrediction using both all sensor's variables and only four most important give same result on testing set pml-training.csv:\r\n\r\n    (pred_tst = predict(modFit,newdata=test_set))\r\n\r\n    1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20\r\n    B  A  B  A  A  E  D  B  A  A  B  C  B  A  E  E  A  B  B  B\r\n\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}